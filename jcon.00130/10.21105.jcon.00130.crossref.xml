<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>ca51b34a09ca68f414c41fed86955a80</doi_batch_id>
    <timestamp>20230811143801</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>JuliaCon Proceedings</full_title>
        <abbrev_title>JCON</abbrev_title>
        <issn media_type="electronic">2642-4029</issn>
        <doi_data>
          <doi>10.21105/jcon</doi>
          <resource>https://proceedings.juliacon.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>08</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>1</volume>
        </journal_volume>
        <issue>1</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Explaining Black-Box Models through Counterfactuals</title>
        </titles>
        
        <publication_date>
          <month>08</month>
          <day>11</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>130</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/jcon.00130</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">https://doi.org/10.5281/zenodo.8239379</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/JuliaCon/proceedings-review/issues/130</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/jcon.00130</doi>
          <resource>https://proceedings.juliacon.org/papers/10.21105/jcon.00130</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://proceedings.juliacon.org/papers/10.21105/jcon.00130.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <doi>10.48550/arXiv.1708.07747</doi>
          </citation>
          <citation key="ref2">
            <unstructured_citation>Altmeyer, Patrick and Angela, Giovan and Buszydlik, Aleksander and Dobiczek, Karol and van Deursen, Arie and Liem, Cynthia, First IEEE Conference on Secure and Trustworthy Machine Learning, Endogenous Macrodynamics in Algorithmic Recourse, :altmeyerendogenous - Endogenous Macrodynamics in Algorithmic Recourse.pdf:PDF, 2023</unstructured_citation>
          </citation>
          <citation key="ref3">
            <unstructured_citation>Antor&#xE1;n, Javier and Bhatt, Umang and Adel, Tameem and Weller, Adrian and Hern&#xE1;ndez-Lobato, Jos&#xE9; Miguel, Getting a Clue: A Method for Explaining Uncertainty Estimates, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2006.06848, arxiv, 2020</unstructured_citation>
          </citation>
          <citation key="ref4">
            <unstructured_citation>Arrieta, Alejandro Barredo and Diaz-Rodriguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and others, Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI, 82&#x2013;115, 58, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Information Fusion, 2020</unstructured_citation>
          </citation>
          <citation key="ref5">
            <doi>10.21105/joss.02704</doi>
          </citation>
          <citation key="ref6">
            <unstructured_citation>Borch, Christian, Machine Learning, Knowledge Risk, and Principal-Agent Problems in Automated Trading, 101852, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Technology in Society, 2022</unstructured_citation>
          </citation>
          <citation key="ref7">
            <unstructured_citation>Buolamwini, Joy and Gebru, Timnit, Conference on Fairness, Accountability and Transparency, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 77&#x2013;91, PMLR, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2018</unstructured_citation>
          </citation>
          <citation key="ref8">
            <unstructured_citation>Fan, Fenglei and Xiong, Jinjun and Wang, Ge, On Interpretability of Artificial Neural Networks, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2001.02522, arxiv, 2020</unstructured_citation>
          </citation>
          <citation key="ref9">
            <unstructured_citation>Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian, Explaining and Harnessing Adversarial Examples, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 1412.6572, arxiv, 2014</unstructured_citation>
          </citation>
          <citation key="ref10">
            <unstructured_citation>Hoffman, Hans, German Credit Data, https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data), https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data), 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 1994</unstructured_citation>
          </citation>
          <citation key="ref11">
            <unstructured_citation>Innes, Mike, Flux: Elegant Machine Learning with Julia, 25, 602, 3, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Journal of Open Source Software, 2018</unstructured_citation>
          </citation>
          <citation key="ref12">
            <unstructured_citation>Joshi, Shalmali and Koyejo, Oluwasanmi and Vijitbenjaronk, Warut and Kim, Been and Ghosh, Joydeep, Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 1907.09615, arxiv, 2019</unstructured_citation>
          </citation>
          <citation key="ref13">
            <unstructured_citation>Kaggle, Give Me Some Credit, Improve on the State of the Art in Credit Scoring by Predicting the Probability That Somebody Will Experience Financial Distress in the next Two Years., https://www.kaggle.com/c/GiveMeSomeCredit, https://www.kaggle.com/c/GiveMeSomeCredit, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Kaggle, 2011</unstructured_citation>
          </citation>
          <citation key="ref14">
            <unstructured_citation>Karimi, Amir-Hossein and Von K&#xFC;gelgen, Julius and Sch&#xF6;lkopf, Bernhard and Valera, Isabel, Algorithmic Recourse under Imperfect Causal Knowledge: A Probabilistic Approach, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2006.06831, arxiv, 2020</unstructured_citation>
          </citation>
          <citation key="ref15">
            <unstructured_citation>Karimi, Amir-Hossein and Barthe, Gilles and Sch&#xF6;lkopf, Bernhard and Valera, Isabel, A Survey of Algorithmic Recourse: Definitions, Formulations, Solutions, and Prospects, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2010.04050, arxiv, 2020</unstructured_citation>
          </citation>
          <citation key="ref16">
            <unstructured_citation>Karimi, Amir-Hossein and Sch&#xF6;lkopf, Bernhard and Valera, Isabel, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, Algorithmic Recourse: From Counterfactual Explanations to Interventions, 353&#x2013;362, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2021</unstructured_citation>
          </citation>
          <citation key="ref17">
            <unstructured_citation>Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer, Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, Interpreting Interpretability: Understanding Data Scientists&#x2019; Use of Interpretability Tools for Machine Learning, 1&#x2013;14, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2020</unstructured_citation>
          </citation>
          <citation key="ref18">
            <unstructured_citation>Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles, Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 1612.01474, arxiv, 2016</unstructured_citation>
          </citation>
          <citation key="ref19">
            <unstructured_citation>LeCun, Yann, The MNIST Database of Handwritten Digits, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, http://yann. lecun. com/exdb/mnist/, 1998</unstructured_citation>
          </citation>
          <citation key="ref20">
            <unstructured_citation>Miller, Tim, Explanation in Artificial Intelligence: Insights from the Social Sciences, 1&#x2013;38, 267, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Artificial intelligence, 2019</unstructured_citation>
          </citation>
          <citation key="ref21">
            <unstructured_citation>Molnar, Christoph, Interpretable Machine Learning, Lulu. com, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2020</unstructured_citation>
          </citation>
          <citation key="ref22">
            <unstructured_citation>Mothilal, Ramaravind K and Sharma, Amit and Tan, Chenhao, Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations, 607&#x2013;617, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2020</unstructured_citation>
          </citation>
          <citation key="ref23">
            <unstructured_citation>O&#x2019;Neil, Cathy, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, Crown, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2016</unstructured_citation>
          </citation>
          <citation key="ref24">
            <unstructured_citation>Pace, R Kelley and Barry, Ronald, Sparse Spatial Autoregressions, 3, 291&#x2013;297, 33, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Statistics &amp; Probability Letters, 1997</unstructured_citation>
          </citation>
          <citation key="ref25">
            <unstructured_citation>Pawelczyk, Martin and Bielawski, Sascha and van den Heuvel, Johannes and Richter, Tobias and Kasneci, Gjergji, Carla: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2108.00783, arxiv, 2021</unstructured_citation>
          </citation>
          <citation key="ref26">
            <unstructured_citation>Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, FACE: Feasible and Actionable Counterfactual Explanations, 344&#x2013;350, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2020</unstructured_citation>
          </citation>
          <citation key="ref27">
            <unstructured_citation>Rudin, Cynthia, Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead, 5, 206&#x2013;215, 1, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Nature Machine Intelligence, 2019</unstructured_citation>
          </citation>
          <citation key="ref28">
            <unstructured_citation>Schut, Lisa and Key, Oscar and Mc Grath, Rory and Costabello, Luca and Sacaleanu, Bogdan and Gal, Yarin and others, International Conference on Artificial Intelligence and Statistics, Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties, 1756&#x2013;1764, PMLR, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2021</unstructured_citation>
          </citation>
          <citation key="ref29">
            <unstructured_citation>Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, Fooling Lime and Shap: Adversarial Attacks on Post Hoc Explanation Methods, 180&#x2013;186, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2020</unstructured_citation>
          </citation>
          <citation key="ref30">
            <unstructured_citation>Slack, Dylan and Hilgard, Anna and Lakkaraju, Himabindu and Singh, Sameer, Counterfactual Explanations Can Be Manipulated, 34, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Advances in Neural Information Processing Systems, 2021</unstructured_citation>
          </citation>
          <citation key="ref31">
            <unstructured_citation>Sturm, Bob L, A Simple Method to Determine If a Music Information Retrieval System Is a &#x201C;Horse&#x201D;, 6, 1636&#x2013;1644, 16, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, IEEE Transactions on Multimedia, 2014</unstructured_citation>
          </citation>
          <citation key="ref32">
            <unstructured_citation>Upadhyay, Sohini and Joshi, Shalmali and Lakkaraju, Himabindu, Towards Robust and Reliable Algorithmic Recourse, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2102.13620, arxiv, 2021</unstructured_citation>
          </citation>
          <citation key="ref33">
            <unstructured_citation>Ustun, Berk and Spangher, Alexander and Liu, Yang, Proceedings of the Conference on Fairness, Accountability, and Transparency, Actionable Recourse in Linear Classification, 10&#x2013;19, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2019</unstructured_citation>
          </citation>
          <citation key="ref34">
            <unstructured_citation>Varshney, Kush R., Trustworthy Machine Learning, Independently Published, Chappaqua, NY, USA, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2022</unstructured_citation>
          </citation>
          <citation key="ref35">
            <unstructured_citation>Verma, Sahil and Dickerson, John and Hines, Keegan, Counterfactual Explanations for Machine Learning: A Review, arXiv, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, 2010.10596, arxiv, 2020</unstructured_citation>
          </citation>
          <citation key="ref36">
            <unstructured_citation>Wachter, Sandra and Mittelstadt, Brent and Russell, Chris, Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR, 841, 31, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Harv. JL &amp; Tech., 2017</unstructured_citation>
          </citation>
          <citation key="ref37">
            <unstructured_citation>Yeh, I-Cheng and Lien, Che-hui, The Comparisons of Data Mining Techniques for the Predictive Accuracy of Probability of Default of Credit Card Clients, 2, 2473&#x2013;2480, 36, 2022-12-13 12:58:01 +0100, 2022-12-13 12:58:01 +0100, Expert systems with applications, 2009</unstructured_citation>
          </citation>
          <citation key="ref38">
            <unstructured_citation>Pawelczyk, Martin and Datta, Teresa and van-den-Heuvel, Johannes and Kasneci, Gjergji and Lakkaraju, Himabindu, Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse, :pawelczyk2022probabilistically - Probabilistically Robust Recourse_ Navigating the Trade Offs between Costs and Robustness in Algorithmic Recourse.pdf:PDF, arXiv preprint arXiv:2203.06768, Probabilistically Robust Recourse, 2022</unstructured_citation>
          </citation>
          <citation key="ref39">
            <unstructured_citation>Krizhevsky, A., 2009, Learning Multiple Layers of Features from Tiny Images, https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086, 2023-06-21, Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images., [TLDR] It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network., Semantic Scholar Link:https\://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086:text/html;Full Text PDF:http\://www.cs.toronto.edu/&#xA0;kriz/learning-features-2009-TR.pdf:application/pdf</unstructured_citation>
          </citation>
          <citation key="ref40">
            <doi>10.24432/C5XW20</doi>
          </citation>
          <citation key="ref41">
            <doi>10.1145/3097983.3098039</doi>
          </citation>
          <citation key="ref42">
            <unstructured_citation>Dandl, Susanne and Hofheinz, Andreas and Binder, Martin and Bischl, Bernd and Casalicchio, Giuseppe, 2023-04, arXiv, counterfactuals: An R Package for Counterfactual Explanation Methods, arXiv:2304.06569 [cs, stat] type: article, http://arxiv.org/abs/2304.06569, 2023-06-21, Counterfactual explanation methods provide information on how feature values of individual observations must be changed to obtain a desired prediction. Despite the increasing amount of proposed methods in research, only a few implementations exist whose interfaces and requirements vary widely. In this work, we introduce the counterfactuals R package, which provides a modular and unified R6-based interface for counterfactual explanation methods. We implemented three existing counterfactual explanation methods and propose some optional methodological extensions to generalize these methods to different scenarios and to make them more comparable. We explain the structure and workflow of the package using real use cases and show how to integrate additional counterfactual explanation methods into the package. In addition, we compared the implemented methods for a variety of models and datasets with regard to the quality of their counterfactual explanations and their runtime behavior., arXiv Fulltext PDF:https\://arxiv.org/pdf/2304.06569.pdf:application/pdf, Statistics - Machine Learning, Computer Science - Machine Learning, Statistics - Computation, counterfactuals</unstructured_citation>
          </citation>
          <citation key="ref43">
            <doi>10.48550/arXiv.1712.08443</doi>
          </citation>
          <citation key="ref44">
            <unstructured_citation>Delaney, Eoin and Greene, Derek and Keane, Mark T., 2021-07, arXiv, Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions, arXiv:2107.09734 [cs] type: article, http://arxiv.org/abs/2107.09734, 2023-06-23, Whilst an abundance of techniques have recently been proposed to generate counterfactual explanations for the predictions of opaque black-box systems, markedly less attention has been paid to exploring the uncertainty of these generated explanations. This becomes a critical issue in high-stakes scenarios, where uncertain and misleading explanations could have dire consequences (e.g., medical diagnosis and treatment planning). Moreover, it is often difficult to determine if the generated explanations are well grounded in the training data and sensitive to distributional shifts. This paper proposes several practical solutions that can be leveraged to solve these problems by establishing novel connections with other research works in explainability (e.g., trust scores) and uncertainty estimation (e.g., Monte Carlo Dropout). Two experiments demonstrate the utility of our proposed solutions., arXiv Fulltext PDF:https\://arxiv.org/pdf/2107.09734.pdf:application/pdf, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
